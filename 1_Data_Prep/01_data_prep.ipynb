{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4049db1-46c7-41a7-8de5-c9c5311a680f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a delta database\n",
    "First we need a place to store our data. Databricks Unity Catalog organizes data in a three-level namespace or format:\n",
    "1. Catalog\n",
    "1. Datbase (aka Schema)\n",
    "1. Table\n",
    "\n",
    "This makes it easy for Databricks Lakehouse AI and Unity Governance to function. This also provides an idea of structure to architects without locking anything in too much in terms of design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02953f5e-83e5-4727-9ce9-875f1c518cf4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declare our variables for our SQL Server"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--TODO: Update the following with your own catalog and database names\n",
    "--SQL is annoying - we'll have to update these manually for now.\n",
    "\n",
    "--Update the catalog name with your own\n",
    "CREATE CATALOG IF NOT EXISTS ademianczuk;\n",
    "\n",
    "--Update the catalog & database name with your own\n",
    "CREATE DATABASE IF NOT EXISTS ademianczuk.ncr;\n",
    "\n",
    "--Update the catalog & database name with your own (leave `data` alone for this notebook. It's just the name of your volume)\n",
    "CREATE VOLUME IF NOT EXISTS ademianczuk.ncr.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "294f570e-ff83-4d6e-a90c-6d932be30020",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Declare our variables for this notebook"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Update the following with your own catalog and database names\n",
    "catalog = \"ademianczuk\"\n",
    "database = \"ncr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a56b2af-aecb-484e-94a2-4175af8d4b91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading in our source data\n",
    "I've included the original source data in the root of this project repository. We're also going to be reading it from a public URL to show how we can source data from pretty much anywhere. We're going to show examples of both reading from a datbricks volume (that may be mapped to a cloud storage container) as well as the same data from a public URL to give you an idea of different ways to ingest source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96671892-29f3-465d-baf9-24a5d1bd250e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Downloading our source data"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "\n",
    "#TODO: Update the following with your own catalog and database names\n",
    "CATALOG=ademianczuk\n",
    "DATABASE=ncr\n",
    "\n",
    "#Create a temp storage location for our downloaded file\n",
    "rm -rf /tmp/ncr || true\n",
    "mkdir -p /tmp/ncr\n",
    "cd /tmp/ncr\n",
    "\n",
    "#Download & extract the gardening archive\n",
    "curl -L https://raw.githubusercontent.com/andrijdemianczuk/uber_analytics/refs/heads/main/ncr_ride_bookings.csv -o ncr_ride_bookings.csv\n",
    "\n",
    "#Move the dataset to our main bucket. Since we're using the root volume directory, we can't manage it with normal sh commands, but for example we'll show it here for posterity. Downloading the same file again will simply overwrite the old one.\n",
    "\n",
    "# rm -rf /Volumes/$CATALOG/$DATABASE/data/ || true\n",
    "# mkdir -p /Volumes/$CATALOG/$DATABASE/data/\n",
    "cp -f ncr_ride_bookings.csv /Volumes/$CATALOG/$DATABASE/data/\n",
    "\n",
    "rm -rf /Volumes/$CATALOG/$DATABASE/data/csv || true\n",
    "mkdir -p /Volumes/$CATALOG/$DATABASE/data/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e51c71-600e-401e-8f2c-5d6e1dd73a7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import our required libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import year, month, dayofweek\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ab16e4-3881-488a-9e36-b0e030308869",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading data stored in a Databricks Volume"
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"multiline\", True)\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"/Volumes/{catalog}/{database}/data/ncr_ride_bookings.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69f70869-4e90-4919-b182-56fd7ba7bb13",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Testing out our data with filters and ordering"
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    df.filter((df[\"Date\"] >= \"2024-01-01\") & (df[\"Date\"] <= \"2024-01-31\")).orderBy(\n",
    "        df[\"Date\"].asc(), df[\"Time\"].asc()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af504dbb-d13e-466b-8e87-369becf67fc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get an idea of my data size"
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80df3361-c632-4b44-a378-79914b803823",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755874250207}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Get our date range"
    }
   },
   "outputs": [],
   "source": [
    "display(df.agg(min(\"Date\"), max(\"Date\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f4b97db-61ef-4be6-8507-a5e4138c7961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Organizing our data\n",
    "Let's break up the dataframe by month (or by week) and write out the parquet files accordingly. For the lab, we'll be separating our data into a number of files so that we can 'drop' them in to our system as a simulation of real data coming in to some type of cloud storage (e.g., ADLS, S3 or GCS buckets). We'll be using Databricks Volumes for this lab. Databricks recommends mapping these Volumes to your cloud storage container whenever possible, however it's not necessary and you can connect to your storage connectors externally if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b48d1fd-7846-45f5-81cf-bb7c024def3e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adding enrichment data columns"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn('dayOfWeek', dayofweek(col('Date')))\n",
    "df = df.withColumn('dayOfMonth', dayofmonth(col('Date')))\n",
    "df = df.withColumn('month', month(col('Date')))\n",
    "df = df.withColumn('year', year(col('Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c288dfb-4201-4e5a-89a0-08c24ee62841",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756135803984}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Previewing our data"
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "034c47b2-68ce-4d84-bced-1e3cc74a9a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Why write to parquet?\n",
    "Parquet is a storage and performance efficient file format. Since it is columnar in nature, it is considered to be a 'dense' storage format. Parquet also allows for data partitioning making it very fast and efficient for filtering data when reading in (since partitions are stored in directories, parquet files that aren't part of the search terms can be ignored). This concept is important to understanding how Delta works as well. Delta is very similar to parquet but also includes metadata at the parent directory level about how data is stored and organized.\n",
    "\n",
    "Although we don't need to write to parquet here, it's good to show an example of how it works. In reality, we'd just read in the raw data to a dataframe or some type of materialization and start working with our data from that point on. Another thing that parquet allows us to do is 'pick up' our workload from this point on in the notebook if we terminate our cluster. Think of it as a 'save state' for our work. Delta works even better for this, so we'll be showing both examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b466635b-4cde-4eb5-b9b1-41147c2040c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Simple parquet patitioning"
    }
   },
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year\", \"month\").mode(\"overwrite\").format(\"parquet\").save(f\"/Volumes/{catalog}/{database}/data/ncr_ride_bookings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597955b9-114c-4c12-8193-45cad08aee8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example reading a parquet partition"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(f\"/Volumes/{catalog}/{database}/data/ncr_ride_bookings/year=2024/month=1\")\n",
    "display(df)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868f2c72-26b4-451e-a074-82dceb2018b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read the entire dataframe back in (we want all our data)"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(f\"/Volumes/{catalog}/{database}/data/ncr_ride_bookings\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9149d529-2f8c-4473-86a1-455efde854bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a number of files to simulate incremental ingestion\n",
    "Now that we have everything organized, let's split up our dataframe into a csv each representing a month's worth of data. We will incrementally load each file in the pipeline later on and will give us an idea of how merging works along with in-flight etl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7002a8d7-752a-4867-bb7a-4babe99f89f3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Split up our data into smaller CSVs for incremental loading"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = f\"/Volumes/{catalog}/{database}/data/csv\"\n",
    "\n",
    "#Get distinct months (assumes 'month' is 1..12; if it's yyyy-MM use that string directly)\n",
    "months = [r.month for r in df.select(\"month\").distinct().collect()]\n",
    "\n",
    "def write_month_csv(src_df: DataFrame, month_val):\n",
    "    \n",
    "    #zero-pad to 2 digits if month is numeric; adjust to your format\n",
    "    m_str = f\"{int(month_val):02d}\" if isinstance(month_val, (int,)) else str(month_val)\n",
    "    tmp_dir = f\"{base_dir}/_tmp_month_{m_str}\"\n",
    "\n",
    "    #1. write to a temp directory with a single part file\n",
    "    (src_df\n",
    "        .filter(col(\"month\") == month_val)\n",
    "        .coalesce(1)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\n",
    "        .csv(tmp_dir))\n",
    "\n",
    "    #2. find the single part file\n",
    "    part_files = [f.path for f in dbutils.fs.ls(tmp_dir) if f.name.startswith(\"part-\") and f.name.endswith(\".csv\")]\n",
    "    if not part_files:\n",
    "        raise RuntimeError(f\"No CSV part file found for month {m_str} in {tmp_dir}\")\n",
    "    part_path = part_files[0]\n",
    "\n",
    "    # 3. move/rename to final destination (flat structure)\n",
    "    final_path = f\"{base_dir}/month_{m_str}.csv\"\n",
    "    dbutils.fs.mv(part_path, final_path, True)\n",
    "\n",
    "    #4. clean up the temp directory\n",
    "    dbutils.fs.rm(tmp_dir, True)\n",
    "\n",
    "for m in months:\n",
    "    write_month_csv(df, m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25983394-8751-4663-97ee-4115e392c8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps\n",
    "Now that we have our data all ready to go, we'll create a small application that simulates dropping each file into another Databricks volume for ingestion. This volume will simulate being attached to an external storage container such as ADLS, GCS or S3."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5608082612136687,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_data_prep",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
