{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b6e9de-ea47-45d5-bc79-2c51a5bef3b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "The actual file copy & update"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import re\n",
    "\n",
    "#TODO: Replace the following variables with your catalog and database name\n",
    "catalog = \"ademianczuk\"\n",
    "database = \"ncr\"\n",
    "\n",
    "#Create the variables for our storage locations\n",
    "table = f\"{catalog}.{database}.csv_copy_progress\"\n",
    "src_dir = f\"/Volumes/{catalog}/{database}/data/csv\"\n",
    "dst_dir = f\"/Volumes/{catalog}/{database}/data/loader\"\n",
    "\n",
    "#Create the destination directory if it doesn't exist. This is what we'll be using for our ingestion pipeline example.\n",
    "dbutils.fs.mkdirs(dst_dir)\n",
    "\n",
    "#Create a table to track file copy progress. This allows us to copy the files one at a time, simulating a periodic file drop.\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table} (\n",
    "  filename STRING,\n",
    "  seq BIGINT,\n",
    "  status STRING,\n",
    "  copied_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "#Validate the files by name\n",
    "def is_month_csv(name):\n",
    "    return name.startswith(\"month_\") and name.endswith(\".csv\")\n",
    "\n",
    "#Validate the file key (month number)\n",
    "def month_key(name):\n",
    "    m = re.search(r\"month[_-](\\d{2})\\.csv$\", name)\n",
    "    return int(m.group(1)) if m else 9999\n",
    "\n",
    "#Get a list of the files in the source directory\n",
    "src_files = [f for f in dbutils.fs.ls(src_dir) if is_month_csv(f.name)]\n",
    "src_files = sorted(src_files, key=lambda f: month_key(f.name))\n",
    "\n",
    "#If no source files exist, raise an elegant error message\n",
    "if not src_files:\n",
    "    raise RuntimeError(f\"No month_*.csv files found in {src_dir}. Have you populated your source directory with data?\")\n",
    "\n",
    "#Compute the next sequence number by counting distinct filenames already logged as \"done\"\n",
    "done = spark.table(table).filter(col(\"status\") == \"done\").select(\"filename\").distinct().collect()\n",
    "done_set = {r.filename for r in done}\n",
    "\n",
    "#Determine the next file (first in order not yet “done”)\n",
    "pending = [f for f in src_files if f.name not in done_set]\n",
    "\n",
    "if not pending:\n",
    "    print(f\"Nothing to do. All {len(src_files)} files already copied.\")\n",
    "else:\n",
    "    src = pending[0]\n",
    "    dst = f\"{dst_dir}/{src.name}\"\n",
    "\n",
    "    #Idempotent copy\n",
    "    exists = False\n",
    "    try:\n",
    "        _ = dbutils.fs.ls(dst)\n",
    "        exists = True\n",
    "    except Exception:\n",
    "        exists = False\n",
    "\n",
    "    if not exists:\n",
    "        dbutils.fs.cp(src.path, dst)\n",
    "        status = \"done\"\n",
    "        print(f\"Copied: {src.path}  ->  {dst}\")\n",
    "    else:\n",
    "        status = \"already_present\"\n",
    "        print(f\"Already present, logging and skipping: {dst}\")\n",
    "\n",
    "    #Append a row to the tracking table (csv_copy_progress)\n",
    "    seq_num = 1 + spark.table(table).count()\n",
    "    df = spark.createDataFrame(\n",
    "        [Row(filename=src.name, seq=seq_num, status=status)]\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"copied_at\", expr(\"current_timestamp()\")\n",
    "    )\n",
    "\n",
    "    df.write.mode(\"append\").saveAsTable(table)\n",
    "\n",
    "    remaining = len(pending) - 1\n",
    "    print(f\"Progress: {len(done_set) + 1}/{len(src_files)} copied. {remaining} remaining.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aaf1b78-cfc6-43f8-8527-7ec7234a27fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reset the loader"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "DROP TABLE IF EXISTS {table}\n",
    "\"\"\")\n",
    "\n",
    "dbutils.fs.rm(dst_dir, True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_ingest_file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
