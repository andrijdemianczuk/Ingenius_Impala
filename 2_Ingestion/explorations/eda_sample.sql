-- Databricks notebook source
-- MAGIC %md
-- MAGIC # Exploratory Notebook
-- MAGIC
-- MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
-- MAGIC
-- MAGIC **Note**: This notebook is not executed as part of the pipeline.
-- MAGIC
-- MAGIC ## How to Use and Exploratory Notebook
-- MAGIC
-- MAGIC This notebook is a good place to test out active queries against data within our pipeline. In most cases both Python and SQL can be executed here against the mid-stream Lakeflow objects. This is a good place to vet queries, joins, and other sequential ETL logic you want to apply to your pipeline.
-- MAGIC

-- COMMAND ----------

-- MAGIC %md
-- MAGIC Run the next cell first. This creates an editable widget that comes pre-populated with the database variables for this lab. You can edit these variables at any time if you want to join or work with data outside the context of the lab.

-- COMMAND ----------

-- DBTITLE 1,Initialize the widgets
-- MAGIC %python
-- MAGIC import hashlib, base64
-- MAGIC
-- MAGIC current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()
-- MAGIC hash_object = hashlib.sha256(current_user.encode())
-- MAGIC hash_user_id = base64.b32encode(hash_object.digest()).decode("utf-8").rstrip("=")[:12]  #Trim to 12 chars for readability
-- MAGIC initials = "".join([x[0] for x in current_user.split("@")[0].split(".")])
-- MAGIC short_hash = hashlib.md5(current_user.encode()).hexdigest()[:8]  #Short 8-char hash
-- MAGIC safe_user_id = f"{initials.upper()}_{short_hash}"
-- MAGIC
-- MAGIC print(safe_user_id)
-- MAGIC
-- MAGIC catalog = "main"
-- MAGIC database = safe_user_id
-- MAGIC volume = "data"
-- MAGIC
-- MAGIC dbutils.widgets.text("catalog", f"{catalog}")
-- MAGIC dbutils.widgets.text("database", f"{database}")
-- MAGIC
-- MAGIC catalog = dbutils.widgets.get("catalog")
-- MAGIC database = dbutils.widgets.get("database")

-- COMMAND ----------

-- MAGIC %md
-- MAGIC **_Important!_**
-- MAGIC
-- MAGIC For the sake of this project, the queries below rely on some tables from the pipeline to exist. In normal circumstances this would happen iteratively and more queries and cells would be added as pipeline development progresses, however for the sake of demonstration **the full pipeline needs to be run at least once to avoid errors**.
-- MAGIC
-- MAGIC _If you are unsure:_ You can always dry run the pipeline first to validate the structure and ensure that everything would operate correctly. If a dry run is successful you can go ahead and run the full pipeline.

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ## Sample EDA
-- MAGIC
-- MAGIC _Andrij's Note:_<br/>
-- MAGIC Below are some examples of how we might want to 'use' the data as a consumer. I typically put myself in the position of the customer or the end user of the data product and imagine myself asking typical business questions such as:
-- MAGIC * How many records are on file?
-- MAGIC * What is the spread or distribution of the records?
-- MAGIC * Does the shape of my data tell me anything interesting?

-- COMMAND ----------

SELECT COUNT(*) as count FROM IDENTIFIER(:catalog || '.' || :database || '.s_updated_wind')

-- COMMAND ----------

-- MAGIC %md
-- MAGIC In the next cell I'm trying to get an idea of the difference between the observed values and the predicted values. This often gives the consumer an idea of how close and reliable predictions are to their target. Plotting this out, we can usually get a good idea of what the residual between the two values looks like.

-- COMMAND ----------

-- DBTITLE 1,View the median and mode values
select
  *,
  ((OPT - ACTUAL)) as pred_diff,
  (OPT / MCR) * 100 as pred_perc,
  (ACTUAL / MCR) * 100 as actual_perc
from
  IDENTIFIER(:catalog || '.' || :database || '.s_updated_wind')
order by
  FORECAST_DATE_LOCAL asc

-- COMMAND ----------

-- MAGIC %md
-- MAGIC This last cell gives us an idea of what we need to build for standard statistics and metrics around the actual observed values. This kind of data usually serves as the basis for business decisions. Since we're also grouping all of the observed metrics by day, this type of aggregation is a perfect candidate for a serving table. We'll use this logic in the last stages of our pipeline so dashboards and reports don't need to recompile this - the pipeline will handle that task instead.

-- COMMAND ----------

-- DBTITLE 1,More Complex Statistics
WITH per_day_stats AS (
  SELECT
    DATE(FORECAST_DATE_LOCAL) AS day,
    COUNT(*) AS n,
    AVG(ACTUAL) AS mean_value,
    STDDEV_SAMP(ACTUAL) AS stddev_value,
    VAR_SAMP(ACTUAL) AS variance_value,
    MIN(ACTUAL) AS min_value,
    MAX(ACTUAL) AS max_value,
    (MAX(ACTUAL) - MIN(ACTUAL)) AS range_value,
    percentile_approx(ACTUAL, 0.25) AS p25,
    percentile_approx(ACTUAL, 0.5) AS median_value,
    percentile_approx(ACTUAL, 0.75) AS p75,
    mode() WITHIN GROUP (ORDER BY ACTUAL) AS mode_value
  FROM IDENTIFIER(:catalog || '.' || :database || '.s_updated_wind')
  GROUP BY DATE(FORECAST_DATE_LOCAL)
)
SELECT
  day,
  n,
  min_value,
  p25,
  median_value,
  mean_value,
  p75,
  max_value,
  (p75 - p25) AS iqr_value,
  stddev_value,
  variance_value,
  range_value,
  mode_value
FROM per_day_stats

